{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "metadata": {
   "interpreter": {
    "hash": "96a5d84734c2fcc0f1c4e29111ed2080412c48bc33f767562c76143427b08200"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Model assembly"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Modell Auswahl\n",
    "\n",
    "Neuronales Netz, but WHY?????\n",
    "\n",
    "## Modellimplementierung"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "###### Imports\n",
    "Importiert die Bibliotheken PyTorch und Pandas"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 1,
   "outputs": []
  },
  {
   "source": [
    "###### Device\n",
    "Bestimmt ob auf der Grafikkarte oder auf dem Prozessor gerechnet werden soll.\n",
    "Wenn eine CUDA fähige Grafikkarte erkannt wird, wird diese als Rechengerät ausgewählt.\n",
    " Dies spart Rechenzeit, da CUDA Kerne deutlich effizienter in der Berechnung Neuronaler Netze sind."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using {} device'.format(device))"
   ]
  },
  {
   "source": [
    "###### PyTorch Klassen\n",
    "Definiert eine Klasse für das Datenset und eine für das Neuronale Netzwerk."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LolProDataset(Dataset):\n",
    "    def __init__(self, data_file):\n",
    "        # Preload data into tensors\n",
    "        data = pd.read_csv(data_file)\n",
    "        self.labels = torch.tensor(data.pop('Win Rate').to_numpy(), device=device)\n",
    "        self.features = torch.tensor(data.to_numpy(), device=device)\n",
    "\n",
    "    def __len__(self):\n",
    "        # Number of rows in the dataset\n",
    "        return self.features.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Returns item (features, label) at specific index\n",
    "        x = self.features[idx]\n",
    "        y = self.labels[idx]\n",
    "        return (x, y)\n",
    "\n",
    "    def split(self, test_rate):\n",
    "        # Returns number of items for test and train sets by given test_rate\n",
    "        testc = int(self.__len__()*test_rate)\n",
    "        trainc = int(self.__len__() - testc)\n",
    "        return [trainc, testc]\n",
    "\n",
    "# Implementierung des neuronalen Netzes auf Basis der Vererbung vom nn.Modul\n",
    "class LolProNetwork(nn.Module):\n",
    "    def __init__(self, network_stack):\n",
    "        super(LolProNetwork, self).__init__()\n",
    "        self.network_stack = network_stack\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network_stack(x)"
   ]
  },
  {
   "source": [
    "###### Train and Test Loops\n",
    "Loop in dem das Neuronale Netz trainiert und getestet wird.\n",
    "Der mittlere Fehler aus jedem Batch wird genutzt um die Gewichte im Netzwerk im nächsten Batch anzupassen.\n",
    "Mit Backpropagation werden die Errors durch das Netzwerk zurückgeführt und die Gewichte entsprechend angepasst werden."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    # Während der Trainingszeit ausgeführt\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Berechnung des Loss durch die Prediction\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y.unsqueeze(1))\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn, optimizer):\n",
    "    # Wird auf dem trainierten Netzwerk während der Testzeit ausgeführt\n",
    "    size = len(dataloader.dataset)\n",
    "    test_loss, err = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y.unsqueeze(1)).item()\n",
    "            err += torch.abs(pred - y.unsqueeze(1)).sum().data\n",
    "    test_loss /= size\n",
    "    err /= size\n",
    "    return (err.item(), test_loss)"
   ]
  },
  {
   "source": [
    "###### Run Code\n",
    "Set parameters, define Network, loss function and optimizer, load in Dataset and train the network\n",
    "The Learning Rate is\n",
    "\n",
    "Die Epochen beschreiben wie oft das training mit anderer batch Reihenfolge durchgeführt werden soll, damit keine Biase\n",
    "zum Ende des Trainingssatzes entstehen. Also wie oft die Trainfunktion auf das Netz ausgeführt werden soll\n",
    "\n",
    "Die Größe der der Batches entscheidet wie viele Zeilen für jede Neuberechnung genutzt werden sollen. Größere Batches können zu Underfitting und kleinere zu Overfitting führen\n",
    "Die Learning Rate ist der Faktor der bestimmt wie weit die Weights in jedem Durchlauf an das vorherige Ergebnis angepasst werden sollen.\n",
    "\n",
    "Der `network_stack` beschreibt den Aufbau des Netzwerkes.\n",
    "Der erste Netzwerklayer wird mit den 20 Datenpunkten gefüttert.\n",
    "Die Aktivierungsfunktion für den Hiddenlayer ist die ReLu Funktion\n",
    "![img](https://pytorch.org/docs/stable/_images/ReLU.png)\n",
    "Die Ausgabe wird mit einer Sigmoid Funktion aktiviert."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lr = 0.0001 # Learning Rate\n",
    "batch_size = 20 # Batch Größe (Parallel berechnete Zeilen)\n",
    "epochs = 10000 # Epochen (Iterationen)\n",
    "\n",
    "# Network\n",
    "network_stack = nn.Sequential(\n",
    "    nn.Linear(20, 5),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(5, 1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "# Erstelle das Model auf basis des Netzwerks und lasse es auf dem Gerät (cpu oder cuda) berechnen\n",
    "model = LolProNetwork(network_stack).to(device).double()\n",
    "\n",
    "loss_fn = nn.L1Loss() # Mean average Loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr) # Optimizer\n",
    "\n",
    "# Datenset laden\n",
    "dataset = LolProDataset('cleanDataTop+MidS10Final.csv')\n",
    "train_data, test_data = random_split(dataset, dataset.split(0.1), generator=torch.Generator().manual_seed(42))\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Training in jeder epoche\n",
    "history = pd.DataFrame([], columns=[\"Epoch\", \"MAE\", \"Loss\"])\n",
    "for t in range(epochs):\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    res = test_loop(test_dataloader, model, loss_fn, optimizer)\n",
    "    history = history.append({\"Epoch\": t+1, \"MAE\": res[0], \"Loss\": res[1]}, ignore_index=True)\n",
    "    if (t+1)%100 == 0:\n",
    "        print(f\"Epoch {t+1} - MAE: {res[0]}, Loss: {res[1]}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "source": [
    "###### Visualisierung\n",
    "Visualiserung des Trainings mit matplotlib"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history['Epoch'], history['MAE'])\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"MAE\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history['Epoch'], history['Loss'])\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My own stuff, gets deleted soon\n",
    "my_test = [100, 5, 4, 1.5, 3.5, 9, 450, 0.5, 0.2, 600, 1.3, 0.6, 0.2, 0.3, 500, 30, 400, 0.3, 0.05, 40]\n",
    "my_test_tensor = torch.tensor(my_test, device=device).double()\n",
    "model(my_test_tensor)"
   ]
  }
 ]
}