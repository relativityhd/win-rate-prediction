{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "metadata": {
   "interpreter": {
    "hash": "96a5d84734c2fcc0f1c4e29111ed2080412c48bc33f767562c76143427b08200"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Model assembly\n",
    "\n",
    "Wir entschieden uns ein Neuronales Netz zu benutzen, da es uns ermöglicht durch Machinelles Lernen eine automatische Gewichtung der verschiedenen Eingangsstatistiken zu bestimmen.\n",
    "\n",
    "## Modellimplementierung\n",
    "\n",
    "### Imports\n",
    "Importiert die Bibliotheken PyTorch und Pandas, sowie Matplotlib um Lernkurven graphisch darzustellen."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "import matplotlib.pyplot as plt"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "###### Device\n",
    "Bestimmt ob auf der Grafikkarte oder auf dem Prozessor gerechnet werden soll.\n",
    "Wenn eine CUDA fähige Grafikkarte erkannt wird, wird diese als Rechengerät ausgewählt.\n",
    " Dies spart Rechenzeit, da CUDA Kerne deutlich effizienter in der Berechnung Neuronaler Netze sind."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using {} device'.format(device))"
   ]
  },
  {
   "source": [
    "### PyTorch Klassen\n",
    "Definiert eine Klasse für das Datenset und eine für das Neuronale Netzwerk."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LolProDataset(Dataset):\n",
    "    def __init__(self, data_file):\n",
    "        # Preload data into tensors\n",
    "        data = pd.read_csv(data_file)\n",
    "        self.labels = torch.tensor(data.pop('Win Rate').to_numpy(), device=device)\n",
    "        self.features = torch.tensor(data.to_numpy(), device=device)\n",
    "\n",
    "    def __len__(self):\n",
    "        # Number of rows in the dataset\n",
    "        return self.features.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Returns item (features, label) at specific index\n",
    "        x = self.features[idx]\n",
    "        y = self.labels[idx]\n",
    "        return (x, y)\n",
    "\n",
    "    def split(self, test_rate):\n",
    "        # Returns number of items for test and train sets by given test_rate\n",
    "        testc = int(self.__len__()*test_rate)\n",
    "        trainc = int(self.__len__() - testc)\n",
    "        return [trainc, testc]\n",
    "\n",
    "# Implementierung des neuronalen Netzes auf Basis der Vererbung vom nn.Modul\n",
    "class LolProNetwork(nn.Module):\n",
    "    def __init__(self, network_stack):\n",
    "        super(LolProNetwork, self).__init__()\n",
    "        self.network_stack = network_stack\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network_stack(x)"
   ]
  },
  {
   "source": [
    "###### Train and Test Loops\n",
    "Loop in dem das Neuronale Netz trainiert und getestet wird.\n",
    "Der mittlere Fehler aus jedem Batch wird genutzt um die Gewichte im Netzwerk im nächsten Batch anzupassen.\n",
    "Mit Backpropagation werden die Errors durch das Netzwerk zurückgeführt und die Gewichte entsprechend angepasst werden."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    # Während der Trainingszeit ausgeführt\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Berechnung des Loss durch die Prediction\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y.unsqueeze(1))\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn, optimizer):\n",
    "    # Wird auf dem trainierten Netzwerk während der Testzeit ausgeführt\n",
    "    size = len(dataloader.dataset)\n",
    "    test_loss, err = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y.unsqueeze(1)).item()\n",
    "            err += torch.abs(pred - y.unsqueeze(1)).sum().data\n",
    "    test_loss /= size\n",
    "    err /= size\n",
    "    return (err.item(), test_loss)"
   ]
  },
  {
   "source": [
    "###### Run Code\n",
    "Set parameters, define Network, loss function and optimizer, load in Dataset and train the network\n",
    "The Learning Rate is\n",
    "\n",
    "Die Epochen beschreiben wie oft das training mit anderer batch Reihenfolge durchgeführt werden soll, damit keine Biase\n",
    "zum Ende des Trainingssatzes entstehen. Also wie oft die Trainfunktion auf das Netz ausgeführt werden soll\n",
    "\n",
    "Die Größe der der Batches entscheidet wie viele Zeilen für jede Neuberechnung genutzt werden sollen. Größere Batches können zu Underfitting und kleinere zu Overfitting führen\n",
    "Die Learning Rate ist der Faktor der bestimmt wie weit die Weights in jedem Durchlauf an das vorherige Ergebnis angepasst werden sollen.\n",
    "\n",
    "Der `network_stack` beschreibt den Aufbau des Netzwerkes.\n",
    "Der erste Netzwerklayer wird mit den 20 Datenpunkten gefüttert.\n",
    "Die Aktivierungsfunktion für den Hiddenlayer ist die ReLu Funktion\n",
    "![img](https://pytorch.org/docs/stable/_images/ReLU.png)\n",
    "Die Ausgabe wird mit einer Sigmoid Funktion aktiviert."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lr = 0.0001 # Learning Rate\n",
    "batch_size = 20 # Batch Größe (Parallel berechnete Zeilen)\n",
    "epochs = 10000 # Epochen (Iterationen)\n",
    "\n",
    "# Network\n",
    "network_stack = nn.Sequential(\n",
    "    nn.Dropout(p=0.2),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(19, 8),\n",
    "    nn.PReLU(),\n",
    "    nn.Linear(8, 1),\n",
    "    nn.Softsign()\n",
    ")\n",
    "\n",
    "# Erstelle das Model auf basis des Netzwerks und lasse es auf dem Gerät (cpu oder cuda) berechnen\n",
    "model = LolProNetwork(network_stack).to(device).double()\n",
    "\n",
    "loss_fn = nn.L1Loss() # Mean average Loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr) # Optimizer\n",
    "\n",
    "# Load Dataset\n",
    "dataset = LolProDataset('cleanData.csv')\n",
    "train_data, test_data = random_split(dataset, dataset.split(0.1), generator=torch.Generator().manual_seed(42))\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Training in jeder Epoche\n",
    "history = pd.DataFrame([], columns=[\"Epoch\", \"MAE\", \"Loss\"])\n",
    "for t in range(epochs):\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    res = test_loop(test_dataloader, model, loss_fn, optimizer)\n",
    "    history = history.append({\"Epoch\": t+1, \"MAE\": res[0], \"Loss\": res[1]}, ignore_index=True)\n",
    "    if (t+1)%100 == 0:\n",
    "        print(f\"Epoch {t+1} - MAE: {res[0]}, Loss: {res[1]}\")"
   ]
  },
  {
   "source": [
    "### Visualize results\n",
    "Mithilfe von Matplotlib werden die Ergebnisse graphisch dargestellt."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotit(history):\n",
    "    fig, axs = plt.subplots(1, 2)\n",
    "\n",
    "    axs[0].plot(history['Epoch'], history['MAE'])\n",
    "    axs[0].set_xlabel(\"Epochs\")\n",
    "    axs[0].set_ylabel(\"MAE\")\n",
    "    axs[1].plot(history['Epoch'], history['Loss'])\n",
    "    axs[1].set_xlabel(\"Epochs\")\n",
    "    axs[1].set_ylabel(\"Loss\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "    plt.show()\n",
    "plotit(history)"
   ]
  },
  {
   "source": [
    "Diese Ergebnisse sehen schon ganz gut aus. Es ist eine deutliche Lernkurve zu erkennen und ein Ergebnis von ca. 89 % Genauigkeit ist auch in Ordnung. Das können wir jedoch noch verbessern!\n",
    "\n",
    "## Grid Search\n",
    "\n",
    "Um eine bessere Architektur und bessere Parameter für das Training zu finden benutzen wir eine Grid-Suche, welche verschiedene Kombinationen aus-testet und die Ergebnisse für spätere Analyse speichert. Hierzu implementieren wir eine Trainer Funktion, welche die verschiedenen Netzwerke und Hyperparameter als Parameter entgegen nimmt und diese austestet."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(lr, batch_size, epochs, network, opt, dataset):\n",
    "    model = LolProNetwork(network).to(device).double()\n",
    "    loss_fn = nn.L1Loss() # MAE Loss function\n",
    "    optimizer = opt(model.parameters(), lr) # Optimizer\n",
    "\n",
    "    dataset = LolProDataset(dataset)\n",
    "    train_data, test_data = random_split(dataset, dataset.split(0.1), generator=torch.Generator().manual_seed(42))\n",
    "    train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
    "    history = pd.DataFrame([], columns=[\"Epoch\", \"MAE\", \"Loss\"])\n",
    "\n",
    "    print(\"\\n-----------------------------------\")\n",
    "    print(f\"\\nStart Training with the Parameters:\\nlearning rate: {lr}\\nbatch size: {batch_size}\\nepochs: {epochs}\\noptimizer: {optimizer}\\nnetwork: {network}\\n\")\n",
    "    for t in range(epochs):\n",
    "        train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "        res = test_loop(test_dataloader, model, loss_fn, optimizer)\n",
    "        history = history.append({\"Epoch\": t+1, \"MAE\": res[0], \"Loss\": res[1]}, ignore_index=True)\n",
    "    \n",
    "    print(f\"\\nSmallest MAE at index {history['MAE'].idxmin()} with:\\n{history['MAE'].min()}\\n\")\n",
    "    plotit(history)\n",
    "    print(\"-----------------------------------\\n\")\n",
    "    return (model, history)\n"
   ]
  },
  {
   "source": [
    "Nun iterieren wir über verschiedene Parameter um die verschiedenen Kombinationen mit der Trainer Funktion testen zu lassen. Dies dauert eine Weile, da jedes Training seine Zeit braucht."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [0.005, 0.0005, 0.00005]\n",
    "batch_sizes = [15, 45, 90]\n",
    "e = 1000\n",
    "optimizer = [torch.optim.Adam, torch.optim.SGD]\n",
    "networks = [\n",
    "    nn.Sequential(\n",
    "        nn.Linear(19, 5),\n",
    "        nn.CELU(),\n",
    "        nn.Linear(5, 1),\n",
    "        nn.Sigmoid()\n",
    "    ),\n",
    "    nn.Sequential(\n",
    "        nn.Linear(19, 64),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(64, 1),\n",
    "        nn.Sigmoid()\n",
    "    )\n",
    "]\n",
    "\n",
    "results = pd.DataFrame([], columns=[\"Learning Rate\", \"Batch Size\", \"Epochs\", \"Optimizer\", \"Network\", \"History\", \"Min MAE\", \"Idx Min MAE\", \"Duration\"])\n",
    " \n",
    "for lr in learning_rates:\n",
    "    for bs in batch_sizes:\n",
    "        if bs == 1 or bs == 10:\n",
    "            continue\n",
    "        for opt in optimizer:\n",
    "            for net in networks:\n",
    "                stime = time.time()\n",
    "                mod, res = trainer(lr, bs, e, net, opt, 'cleanData.csv')\n",
    "                etime = time.time()\n",
    "                results = results.append({\n",
    "                    \"Learning Rate\": lr,\n",
    "                    \"Batch Size\": bs,\n",
    "                    \"Epochs\": e,\n",
    "                    \"Optimizer\": opt,\n",
    "                    \"Network\": net,\n",
    "                    \"History\": res,\n",
    "                    \"Min MAE\": res['MAE'].min(),\n",
    "                    \"Idx Min MAE\": res['MAE'].idxmin(),\n",
    "                    \"Duration\": (etime - stime) * 1000\n",
    "                }, ignore_index=True)\n",
    "results.to_pickle('grid_search_results')\n",
    "results"
   ]
  },
  {
   "source": [
    "Mithilfe von Pandas können wir uns das beste Ergebnis ausgeben lassen."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotit(results.iloc[results['Min MAE'].idxmin()]['History'])\n",
    "results['Min MAE'].min()"
   ]
  },
  {
   "source": [
    "Ein Blick über andere Kombination verrät jedoch, dass andere Kombinationen eventuell viel versprechender sein können als diese, wenn wir sie ein wenig länger Trainieren lassen."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.00005 # Learning Rate\n",
    "batch_size = 45 # Batch Size (Parallel calculated rows)\n",
    "epochs = 3000 # Epochs (iterations over dataset)\n",
    "\n",
    "# Network\n",
    "network_stack = nn.Sequential(\n",
    "    nn.Linear(19, 5),\n",
    "    nn.CELU(),\n",
    "    nn.Linear(5, 1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "model, history = trainer(lr, batch_size, epochs, network_stack, torch.optim.Adam, 'cleanData.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Das sieht schon besser aus. Durch mehr Epochen konnte das Netzwerk immer besser werden. Der Trainingseffekt nimmt jedoch nach einiger Zeit ab und es besteht die Gefahr eines Overfittings. Daher sind 3000 Epochen perfekt für unsere Zwecke."
   ]
  },
  {
   "source": [
    "## Beispiel"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "data = pd.read_csv('cleanData.csv')\n",
    "rates = data.pop('Win Rate')\n",
    "t1 = torch.tensor(data.iloc[idx].to_numpy(), device=device).double().unsqueeze(0)\n",
    "t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rates.iloc[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(t1).item()"
   ]
  },
  {
   "source": [
    "## Modell speichern\n",
    "\n",
    "Um das Modell in einem Webserver oder ähnlichem wieder zu verwenden wird es gespeichert."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './model/lol_predicter_v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}